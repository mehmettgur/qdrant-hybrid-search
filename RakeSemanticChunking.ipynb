{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake_nltk in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rake_nltk) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.65.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake_nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-rake-LLM-data_collection koleksiyonu silindi, yeniden oluşturuluyor...\n",
      "semantic-rake-LLM-data_collection koleksiyonu başarıyla oluşturuldu.\n",
      "136 adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Load configuration\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Qdrant and embedding settings\n",
    "collection_name = \"semantic-rake-LLM-data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=embedding_model_name)\n",
    "\n",
    "# Initialize RAKE for keyword extraction\n",
    "rake_extractor = Rake()\n",
    "\n",
    "# Example SemanticChunking class (your own implementation)\n",
    "chunker = SemanticChunking()\n",
    "\n",
    "# Extract data from PDF\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "# Split into chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.create_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vector_size = 1536\n",
    "\n",
    "# If the collection already exists, delete and recreate it\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} collection deleted, recreating...\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"{collection_name} collection successfully created.\")\n",
    "\n",
    "# While adding chunks to Qdrant, also add RAKE keywords to the payload\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    vector = embeddings.embed_query(chunk)\n",
    "\n",
    "    # Extract keywords from the chunk content with RAKE\n",
    "    rake_extractor.extract_keywords_from_text(chunk)\n",
    "    chunk_keywords = rake_extractor.get_ranked_phrases()[:5]  # Top 5 keywords\n",
    "\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\n",
    "            \"text\": chunk,\n",
    "            \"keywords\": chunk_keywords\n",
    "        }\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(f\"{len(points)} semantic chunks successfully added to Qdrant collection.\")\n",
    "\n",
    "def test_rag_hybrid_qdrant(qdrant_client, collection_name, query):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compute the query embedding\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    # Extract keywords from the query using RAKE\n",
    "    rake_extractor.extract_keywords_from_text(query)\n",
    "    query_keywords = rake_extractor.get_ranked_phrases()[:3]  # Top 3 keywords\n",
    "\n",
    "    # Retrieve 10 candidate results from Qdrant based on cosine similarity\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=10,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    # Post-processing: calculate keyword matching score for each document\n",
    "    re_ranked = []\n",
    "    for result in search_results:\n",
    "        cosine_score = result.score  # Cosine similarity score from Qdrant\n",
    "        payload = result.payload\n",
    "\n",
    "        # Get the document's keyword list (if any)\n",
    "        doc_keywords = payload.get(\"keywords\", [])\n",
    "\n",
    "        # Calculate match ratio between query keywords and document keywords (0-1)\n",
    "        match_count = 0\n",
    "        for kw in query_keywords:\n",
    "            for doc_kw in doc_keywords:\n",
    "                if kw.lower() in doc_kw.lower():\n",
    "                    match_count += 1\n",
    "                    break\n",
    "        keyword_score = match_count / len(query_keywords) if query_keywords else 0\n",
    "\n",
    "        # Hybrid score: 0.7 cosine + 0.3 keyword match\n",
    "        final_score = 0.7 * cosine_score + 0.3 * keyword_score\n",
    "        re_ranked.append((final_score, payload[\"text\"]))\n",
    "\n",
    "    # Sort results by final_score in descending order\n",
    "    re_ranked.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_results = re_ranked[:5]\n",
    "\n",
    "    if not top_results:\n",
    "        print(\"No matching document found in Qdrant.\")\n",
    "        return\n",
    "\n",
    "    # Build context from the best results\n",
    "    context = \"\\n\".join([doc for score, doc in top_results])\n",
    "    print(\"\\nSources used from Qdrant (hybrid ranking):\")\n",
    "    for idx, (final_score, doc) in enumerate(top_results, start=1):\n",
    "        print(f\"{idx}. (Hybrid Score: {final_score:.3f}, Length: {len(doc)}) {doc[:100]}...\")\n",
    "\n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference time: {round(end_time - start_time, 3)} seconds\")\n",
    "    print(f\"\\nModel's answer:\\n{response.choices[0].message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What are the key features of the Transformer architecture?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.554, Uzunluk: 6730) verge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many fa...\n",
      "2. (Hybrid Skor: 0.549, Uzunluk: 3153) This can be expressed as Output = Merge(head 1,...,head τ)Whead(2.70) where head j∈Rdhis computed us...\n",
      "3. (Hybrid Skor: 0.547, Uzunluk: 4147) An example 74 Generative Models of this approach is compressive Transformer [ Rae et al. ,2019 ]. It...\n",
      "4. (Hybrid Skor: 0.545, Uzunluk: 1907) [Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture searc...\n",
      "5. (Hybrid Skor: 0.544, Uzunluk: 4824) So they cannot cover functions with inﬂection points, such as double descent cur ves. In response, r...\n",
      "\n",
      "Inference zamanı: 6.923 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The key features of the Transformer architecture include:\n",
      "\n",
      "1. **Layer Normalization**: Used to stabilize training by normalizing layer outputs, reducing covariate shift, and improving training stability. It can be implemented in two architectures: post-norm (after residual blocks) and pre-norm (inside residual blocks), with pre-norm being more effective for deep Transformers.\n",
      "\n",
      "2. **Residual Connections**: These connections allow gradients to flow through the network more effectively, facilitating the training of deeper models.\n",
      "\n",
      "3. **Feed-Forward Networks (FFNs)**: Introduce non-linearities into representation learning, preventing degeneration of representations learned by self-attention. FFNs typically consist of a linear transformation followed by an activation function.\n",
      "\n",
      "4. **Activation Functions**: Commonly used activation functions include ReLU, GeLU (Gaussian Error Linear Unit), and GLU (Gated Linear Unit) variants, which help in managing non-linearities in the model.\n",
      "\n",
      "5. **Multi-Head Attention**: This mechanism allows the model to focus on different parts of the input sequence simultaneously by using multiple attention heads, each with its own set of queries, keys, and values.\n",
      "\n",
      "6. **Positional Embeddings**: Since Transformers are order-insensitive, positional embeddings are added to token embeddings to provide information about the position of tokens in the sequence.\n",
      "\n",
      "7. **Efficient Attention Mechanisms**: Variants like multi-query attention (MQA) and grouped query attention (GQA) help reduce computational\n"
     ]
    }
   ],
   "source": [
    "query7 = \"What are the key features of the Transformer architecture?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is positional encoding?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.594, Uzunluk: 5334) In this case, the embedding at position ican be expressed as ei=xi+ PE(i) (2.74) where xi∈Rddenotes ...\n",
      "2. (Hybrid Skor: 0.571, Uzunluk: 2879) positions are represented as combinations of sine and cosin e functions with different frequencies. ...\n",
      "3. (Hybrid Skor: 0.568, Uzunluk: 1108) However, Press et al. [2022 ] found that setting βto values decreasing geometrically by a factor of1...\n",
      "4. (Hybrid Skor: 0.561, Uzunluk: 7737) is the rotation matrix. If two or more rotations are performe d on the same vector, we can rotate th...\n",
      "5. (Hybrid Skor: 0.557, Uzunluk: 3860) i+1for short). Suppose we have the gold- standard distribution at the same position, denoted by pgol...\n",
      "\n",
      "Inference zamanı: 6.634 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Positional encoding is a technique used in neural network models, particularly in transformer architectures, to inject information about the position of tokens in a sequence. Since token embeddings are position-independent vectors, positional encodings are added to these embeddings to provide context regarding the order of the tokens. This allows the model to differentiate between tokens that appear at different positions in a sequence.\n",
      "\n",
      "There are various methods for positional encoding, including learned embeddings, sinusoidal functions, and rotary positional embeddings. The sinusoidal encoding method uses sine and cosine functions to create continuous and periodic positional representations that can generalize to sequences of varying lengths. In contrast, learned positional embeddings can lead to issues when the model encounters positions not seen during training. Overall, positional encoding is essential for enabling transformers to understand the sequential nature of input data.\n"
     ]
    }
   ],
   "source": [
    "query10 = \"What is positional encoding?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is semantic chunking and how does it improve search efficiency?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.546, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "2. (Hybrid Skor: 0.544, Uzunluk: 3393) Long Papers) , pages 86–96, 2016. [Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, ...\n",
      "3. (Hybrid Skor: 0.543, Uzunluk: 5572) This moti vates researchers to develop new evaluation benchmarks and metrics for long-context LLMs. ...\n",
      "4. (Hybrid Skor: 0.541, Uzunluk: 3900) it remains challenging to effectively prompt LLMs. Note tha t if we face a very difﬁcult classiﬁca- ...\n",
      "5. (Hybrid Skor: 0.540, Uzunluk: 5692) to operate. To address these challenges, various optimizat ion strategies, such as pruning, quantiza...\n",
      "\n",
      "Inference zamanı: 11.257 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Semantic chunking is a natural language processing technique that involves breaking down text into meaningful segments or \"chunks\" based on the semantic content. This process helps in organizing information in a way that makes it easier for algorithms to understand and retrieve relevant data.\n",
      "\n",
      "By segmenting text into smaller, contextually relevant pieces, semantic chunking improves search efficiency in several ways:\n",
      "\n",
      "1. **Reduced Search Space**: Instead of searching through entire documents, the search can be conducted over smaller, more focused chunks, which reduces the amount of data that needs to be processed.\n",
      "\n",
      "2. **Improved Relevance**: Chunks can be designed to encapsulate specific topics or concepts, allowing search algorithms to retrieve more relevant results based on the user's query.\n",
      "\n",
      "3. **Enhanced Context Awareness**: By understanding the semantic relationships within chunks, search systems can better interpret user intent and provide more accurate results.\n",
      "\n",
      "4. **Facilitated Indexing**: Chunks can be indexed separately, allowing for faster retrieval and more efficient use of storage resources.\n",
      "\n",
      "Overall, semantic chunking enhances the ability of search systems to deliver precise and contextually relevant information, thereby improving user experience and efficiency in information retrieval.\n"
     ]
    }
   ],
   "source": [
    "query12 = \"What is semantic chunking and how does it improve search efficiency?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your custom points have been successfully added to the collection!\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Üç ayrı metin tanımlıyoruz\n",
    "custom_texts = [\n",
    "    \"The Transformer architecture is a fundamental building block in language models. Its self-attention mechanism enables the model to capture relationships between words.\",\n",
    "    \"Positional encoding assists models in understanding the order of words, ensuring that the sentence structure is maintained.\",\n",
    "    \"Semantic chunking splits long texts into meaningful segments, facilitating efficient search and information retrieval. This method helps in grouping similar content together.\"\n",
    "]\n",
    "\n",
    "points = []\n",
    "for text in custom_texts:\n",
    "    # Her metin için embedding hesapla\n",
    "    vector = embeddings.embed_query(text)\n",
    "    \n",
    "    # RAKE ile metinden anahtar kelimeleri çıkar\n",
    "    rake_extractor.extract_keywords_from_text(text)\n",
    "    custom_keywords = rake_extractor.get_ranked_phrases()[:5]\n",
    "    \n",
    "    # Her metin için ayrı point oluştur\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\n",
    "            \"text\": text,\n",
    "            \"keywords\": custom_keywords\n",
    "        }\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "# Tüm point’leri Qdrant koleksiyonuna ekle\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(\"Your custom points have been successfully added to the collection!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What are the key features of the Transformer architecture??\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.756, Uzunluk: 167) The Transformer architecture is a fundamental building block in language models. Its self-attention ...\n",
      "2. (Hybrid Skor: 0.556, Uzunluk: 6730) verge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many fa...\n",
      "3. (Hybrid Skor: 0.547, Uzunluk: 3153) This can be expressed as Output = Merge(head 1,...,head τ)Whead(2.70) where head j∈Rdhis computed us...\n",
      "4. (Hybrid Skor: 0.546, Uzunluk: 4147) An example 74 Generative Models of this approach is compressive Transformer [ Rae et al. ,2019 ]. It...\n",
      "5. (Hybrid Skor: 0.546, Uzunluk: 1925) [Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011. [Kaplan et al., 2020] J...\n",
      "\n",
      "Inference zamanı: 9.71 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The key features of the Transformer architecture include:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This allows the model to capture relationships between words by weighing their relevance to each other, enabling it to focus on different parts of the input sequence as needed.\n",
      "\n",
      "2. **Layer Normalization**: Used to stabilize training by normalizing layer outputs, which reduces covariate shift and improves training stability. It can be implemented in either a post-norm or pre-norm architecture, with the pre-norm architecture being particularly effective for deep Transformers.\n",
      "\n",
      "3. **Feedforward Neural Networks (FFNs)**: These introduce non-linearities into representation learning, preventing degeneration of representations learned by self-attention. Various activation functions, such as ReLU, GeLU, and GLU-based functions, are commonly used in FFNs.\n",
      "\n",
      "4. **Residual Connections**: These connections help in training deep networks by allowing gradients to flow more easily through the layers, mitigating issues like vanishing gradients.\n",
      "\n",
      "5. **Multi-Head Attention**: This allows the model to attend to different parts of the input sequence simultaneously, improving the model's ability to capture various aspects of the input.\n",
      "\n",
      "6. **Positional Encoding**: Since the Transformer architecture is order-insensitive, positional embeddings are added to input token embeddings to provide information about the position of each token in the sequence.\n",
      "\n",
      "7. **Scalability**: The architecture is designed to be scalable, allowing it to handle large datasets and complex tasks efficiently, often\n"
     ]
    }
   ],
   "source": [
    "query8 = \"What are the key features of the Transformer architecture??\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is positional encoding?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.918, Uzunluk: 123) Positional encoding assists models in understanding the order of words, ensuring that the sentence s...\n",
      "2. (Hybrid Skor: 0.594, Uzunluk: 5334) In this case, the embedding at position ican be expressed as ei=xi+ PE(i) (2.74) where xi∈Rddenotes ...\n",
      "3. (Hybrid Skor: 0.571, Uzunluk: 2879) positions are represented as combinations of sine and cosin e functions with different frequencies. ...\n",
      "4. (Hybrid Skor: 0.568, Uzunluk: 1108) However, Press et al. [2022 ] found that setting βto values decreasing geometrically by a factor of1...\n",
      "5. (Hybrid Skor: 0.561, Uzunluk: 7737) is the rotation matrix. If two or more rotations are performe d on the same vector, we can rotate th...\n",
      "\n",
      "Inference zamanı: 6.697 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Positional encoding is a technique used in models, particularly in natural language processing, to help them understand the order of words in a sequence. It ensures that the structure of the sentence is maintained by adding positional information to token embeddings, which are otherwise position-independent. This is typically done by combining token embeddings with positional embeddings, allowing the model to distinguish tokens based on their positions in the sequence. Positional encoding can be learned or designed to generalize, enabling the model to handle longer sequences during inference than those seen during training.\n"
     ]
    }
   ],
   "source": [
    "query9 = \"What is positional encoding?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is semantic chunking and how does it improve search efficiency?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.783, Uzunluk: 174) Semantic chunking splits long texts into meaningful segments, facilitating efficient search and info...\n",
      "2. (Hybrid Skor: 0.564, Uzunluk: 123) Positional encoding assists models in understanding the order of words, ensuring that the sentence s...\n",
      "3. (Hybrid Skor: 0.546, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "4. (Hybrid Skor: 0.544, Uzunluk: 3393) Long Papers) , pages 86–96, 2016. [Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, ...\n",
      "5. (Hybrid Skor: 0.543, Uzunluk: 5572) This moti vates researchers to develop new evaluation benchmarks and metrics for long-context LLMs. ...\n",
      "\n",
      "Inference zamanı: 2.453 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Semantic chunking is a method that splits long texts into meaningful segments or chunks, which facilitates efficient search and information retrieval. By grouping similar content together, it helps users quickly locate relevant information within extensive texts, improving the overall search efficiency. This organization of content allows for better comprehension and retrieval of specific details, as users can focus on smaller, contextually coherent segments rather than sifting through large volumes of unstructured text.\n"
     ]
    }
   ],
   "source": [
    "query11 = \"What is semantic chunking and how does it improve search efficiency?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.870, Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "2. (Hybrid Skor: 0.857, Uzunluk: 3860) i+1for short). Suppose we have the gold- standard distribution at the same position, denoted by pgol...\n",
      "3. (Hybrid Skor: 0.725, Uzunluk: 2710) forgetting problem in continual training, where a neural network forge ts previously learned in- for...\n",
      "4. (Hybrid Skor: 0.722, Uzunluk: 5110) The training objective can be deﬁned as (ˆθ,ˆω) = arg max θ,ωLoss(Model θ,ω(xnoise),x) (1.16) Here t...\n",
      "5. (Hybrid Skor: 0.721, Uzunluk: 4839) The ﬁne-tuned model is then employed to classify new sequences for this task. An advantage of superv...\n",
      "\n",
      "Inference zamanı: 3.179 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training is a process in natural language processing (NLP) where models are initially trained on a large amount of unlabeled data to learn general representations of language. This phase often involves self-supervised learning techniques, where the model generates its own supervision signals from the data. During pre-training, models learn to perform tasks such as predicting masked words or classifying sentences based on certain criteria. After pre-training, these models can be fine-tuned on specific downstream tasks using labeled data to enhance their performance. The goal of pre-training is to create a robust foundation model that can be easily adapted to various NLP applications.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.682, Uzunluk: 9996) D dataset used for training or ﬁne-tuning a model ∂L ∂θgradient of the loss function Lwith respect t...\n",
      "2. (Hybrid Skor: 0.604, Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "3. (Hybrid Skor: 0.592, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "4. (Hybrid Skor: 0.590, Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "5. (Hybrid Skor: 0.589, Uzunluk: 3861) The use of these ex- amples does not distinguish between models, but we mark the m odel architecture...\n",
      "\n",
      "Inference zamanı: 6.824 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The types of models widely used in NLP pre-training include:\n",
      "\n",
      "1. **Encoder-only Models**: Such as BERT, which is trained using tasks like masked language modeling and next sentence prediction.\n",
      "2. **Decoder-only Models**: Such as GPT, which focuses on autoregressive language modeling.\n",
      "3. **Encoder-Decoder Models**: These models are used for tasks that require both understanding and generating sequences, such as translation.\n",
      "\n",
      "These models leverage self-supervised learning on large amounts of unlabeled data to achieve general language understanding and generation capabilities.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.582, Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "2. (Hybrid Skor: 0.582, Uzunluk: 8453) The approach described above provides a new framework of uni versal language understanding and gener...\n",
      "3. (Hybrid Skor: 0.581, Uzunluk: 7725) sense to predict any of the tokens in this sequence. 1.2.2.1 Masked Language Modeling One of the mos...\n",
      "4. (Hybrid Skor: 0.578, Uzunluk: 5839) architecture to adapt LLMs to large-scale training. In Sect ion2.2we will present more discussions o...\n",
      "5. (Hybrid Skor: 0.577, Uzunluk: 5843) ↓ Is Next or Not? 1.2 Self-supervised Pre-training Tasks 13 x0 x0x1 x1x2 x2x3 x3x4 x4Pr(x0) = 1 Pr(x...\n",
      "\n",
      "Inference zamanı: 9.092 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling involves making sequential predictions of tokens in a different order than their natural sequence. In this approach, the actual order of tokens in the text remains unchanged, but the order in which predictions are made is altered. Here’s how to implement permuted language modeling:\n",
      "\n",
      "1. **Select a Sequence**: Start with a sequence of tokens, for example, \\( x_0, x_1, x_2, x_3, x_4 \\).\n",
      "\n",
      "2. **Determine a Prediction Order**: Decide on a new order for token predictions. For instance, instead of predicting in the natural order \\( x_0 \\to x_1 \\to x_2 \\to x_3 \\to x_4 \\), you might choose \\( x_0 \\to x_4 \\to x_2 \\to x_1 \\to x_3 \\).\n",
      "\n",
      "3. **Model the Probability**: The probability of the sequence can be modeled as:\n",
      "   \\[\n",
      "   Pr(x) = Pr(x_0) \\cdot Pr(x_4 | e_0) \\cdot Pr(x_2 | e_0, e_4) \\cdot Pr(x_1 | e_0, e_4, e_2) \\cdot Pr(x_3 | e_0, e_4, e_2, e_1)\n",
      "   \\]\n",
      "   Here, \\( e_i \\) represents the embedding of token \\( x_i\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Skor: 0.683, Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "2. (Hybrid Skor: 0.682, Uzunluk: 4869) scaling laws for LLMs, which help us understand their traini ng efﬁciency and effectiveness. 2.2.1 D...\n",
      "3. (Hybrid Skor: 0.677, Uzunluk: 9182) example, in He et al. [2021 ]’s work, a 1.5 billion-parameter BERT-like model is built b y increasin...\n",
      "4. (Hybrid Skor: 0.674, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "5. (Hybrid Skor: 0.673, Uzunluk: 9992) [Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Mac hel Reid, Yutaka Matsuo, and Yusuke Iwa...\n",
      "\n",
      "Inference zamanı: 5.763 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The large-scale pre-training of the document refers to the process of training large language models (LLMs) on extensive datasets consisting of trillions of tokens. This pre-training typically involves using a sequence model combined with a classification layer to perform tasks such as sentiment classification. The document highlights the importance of data quality, diversity, and the challenges associated with collecting and processing large-scale datasets, including issues of bias and privacy. It also discusses the necessity of fine-tuning these pre-trained models for specific downstream tasks using labeled data to ensure optimal performance. Additionally, the document mentions various models and their respective amounts of training data, emphasizing that larger datasets do not always guarantee better training results.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
