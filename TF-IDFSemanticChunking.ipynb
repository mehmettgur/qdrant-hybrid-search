{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-tfidf-LLM-data_collection koleksiyonu silindi, yeniden oluşturuluyor...\n",
      "semantic-tfidf-LLM-data_collection koleksiyonu başarıyla oluşturuldu.\n",
      "136 adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Helper function that extracts keywords using TF-IDF\n",
    "def extract_keywords_tfidf(text, top_n: int = 5):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_array = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    if len(tfidf_scores) == 0:\n",
    "        return []\n",
    "    # Select the top-n words with the highest scores\n",
    "    top_n_idx = tfidf_scores.argsort()[::-1][:top_n]\n",
    "    top_keywords = [feature_array[i] for i in top_n_idx]\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "# Load configuration\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Qdrant and embedding settings\n",
    "collection_name = \"semantic-tfidf-LLM-data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY, model=embedding_model_name\n",
    ")\n",
    "\n",
    "# Example SemanticChunking class (your own implementation)\n",
    "chunker = SemanticChunking()\n",
    "\n",
    "# Pull data from PDF\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "# Split into chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.create_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vector_size = 1536\n",
    "\n",
    "# If the collection already exists, delete it and recreate\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} collection deleted, recreating...\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"{collection_name} collection successfully created.\")\n",
    "\n",
    "# While inserting chunks into Qdrant, also attach TF-IDF keywords to the payload\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    vector = embeddings.embed_query(chunk)\n",
    "\n",
    "    # Extract keywords from the chunk content using TF-IDF\n",
    "    chunk_keywords = extract_keywords_tfidf(chunk, top_n=5)\n",
    "\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk, \"keywords\": chunk_keywords},\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "print(f\"{len(points)} semantic chunks successfully added to Qdrant collection.\")\n",
    "\n",
    "\n",
    "def test_rag_hybrid_qdrant(qdrant_client, collection_name, query: str):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compute the query embedding\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    # Extract keywords from the query with TF-IDF (top 3 words)\n",
    "    query_keywords = extract_keywords_tfidf(query, top_n=3)\n",
    "\n",
    "    # Retrieve 10 candidate results from Qdrant based on cosine similarity\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=10,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # Post-processing: compute a keyword-match score for each document\n",
    "    re_ranked = []\n",
    "    for result in search_results:\n",
    "        cosine_score = result.score  # Cosine similarity score returned by Qdrant\n",
    "        payload = result.payload\n",
    "\n",
    "        # Get the document's keyword list (if any)\n",
    "        doc_keywords = payload.get(\"keywords\", [])\n",
    "\n",
    "        # Calculate the match ratio between query and document keywords (0-1)\n",
    "        match_count = 0\n",
    "        for kw in query_keywords:\n",
    "            for doc_kw in doc_keywords:\n",
    "                if kw.lower() in doc_kw.lower():\n",
    "                    match_count += 1\n",
    "                    break\n",
    "        keyword_score = match_count / len(query_keywords) if query_keywords else 0\n",
    "\n",
    "        # Hybrid score: 0.7 cosine, 0.3 keyword match\n",
    "        # Additionally keep the raw cosine score\n",
    "        final_score = 0.7 * cosine_score + 0.3 * keyword_score\n",
    "        re_ranked.append((final_score, cosine_score, payload[\"text\"]))\n",
    "\n",
    "    # Sort results in descending order of final_score\n",
    "    re_ranked.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_results = re_ranked[:5]\n",
    "\n",
    "    if not top_results:\n",
    "        print(\"No matching document found in Qdrant.\")\n",
    "        return\n",
    "\n",
    "    # The context variable is built only from the document texts\n",
    "    context = \"\\n\".join([doc for _, _, doc in top_results])\n",
    "    print(\"\\nSources used from Qdrant (hybrid ranking):\")\n",
    "    for idx, (final_score, cosine_score, doc) in enumerate(top_results, start=1):\n",
    "        print(\n",
    "            f\"{idx}. (Hybrid Score: {final_score:.3f}, Cosine: {cosine_score:.3f}, \"\n",
    "            f\"Length: {len(doc)}) {doc[:100]}...\"\n",
    "        )\n",
    "\n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant that provides precise answers based on the given context.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference time: {round(end_time - start_time, 3)} seconds\")\n",
    "    print(f\"\\nModel's answer:\\n{response.choices[0].message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What are the key features of the Transformer architecture?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.554, Cosine: 0.792, Uzunluk: 6730) verge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many fa...\n",
      "2. (Hybrid Score: 0.549, Cosine: 0.784, Uzunluk: 3153) This can be expressed as Output = Merge(head 1,...,head τ)Whead(2.70) where head j∈Rdhis computed us...\n",
      "3. (Hybrid Score: 0.547, Cosine: 0.781, Uzunluk: 4147) An example 74 Generative Models of this approach is compressive Transformer [ Rae et al. ,2019 ]. It...\n",
      "4. (Hybrid Score: 0.545, Cosine: 0.779, Uzunluk: 1907) [Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture searc...\n",
      "5. (Hybrid Score: 0.544, Cosine: 0.778, Uzunluk: 4824) So they cannot cover functions with inﬂection points, such as double descent cur ves. In response, r...\n",
      "\n",
      "Inference zamanı: 7.497 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The key features of the Transformer architecture include:\n",
      "\n",
      "1. **Layer Normalization with Residual Connections**: Utilizes layer normalization to stabilize training, often combined with residual connections. The pre-norm architecture is particularly effective for training deep Transformers.\n",
      "\n",
      "2. **Feedforward Networks (FFNs)**: Introduces non-linearities into representation learning, preventing degeneration of representations learned by self-attention. Common activation functions include ReLU, Gaussian Error Linear Unit (GeLU), and Gated Linear Unit (GLU)-based functions.\n",
      "\n",
      "3. **Multi-head Attention**: Implements attention mechanisms that allow the model to focus on different parts of the input sequence simultaneously. Variants include multi-query attention (MQA) and grouped query attention (GQA), which optimize the efficiency of key-value (KV) caching.\n",
      "\n",
      "4. **Positional Embeddings**: Encodes positional information in input tokens to maintain the order of sequences, often through sinusoidal or rotary positional encodings.\n",
      "\n",
      "5. **Scalability and Efficiency**: Designed to handle large-scale training and long sequences, employing techniques such as low-precision arithmetic and hardware-aware optimizations to improve performance and reduce computational costs.\n",
      "\n",
      "6. **Modifications for Stability**: Includes practices like removing bias terms in transformations and adjusting batch sizes during training to enhance stability and efficiency.\n",
      "\n",
      "7. **Memory Management**: Employs internal and external memory strategies to manage long-context sequences effectively, allowing for the processing of extensive textual contexts without significant loss of information.\n",
      "\n",
      "These\n"
     ]
    }
   ],
   "source": [
    "query7 = \"What are the key features of the Transformer architecture?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is positional encoding?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.744, Cosine: 0.849, Uzunluk: 5334) In this case, the embedding at position ican be expressed as ei=xi+ PE(i) (2.74) where xi∈Rddenotes ...\n",
      "2. (Hybrid Score: 0.718, Cosine: 0.811, Uzunluk: 1108) However, Press et al. [2022 ] found that setting βto values decreasing geometrically by a factor of1...\n",
      "3. (Hybrid Score: 0.711, Cosine: 0.802, Uzunluk: 7737) is the rotation matrix. If two or more rotations are performe d on the same vector, we can rotate th...\n",
      "4. (Hybrid Score: 0.571, Cosine: 0.816, Uzunluk: 2879) positions are represented as combinations of sine and cosin e functions with different frequencies. ...\n",
      "5. (Hybrid Score: 0.557, Cosine: 0.796, Uzunluk: 3860) i+1for short). Suppose we have the gold- standard distribution at the same position, denoted by pgol...\n",
      "\n",
      "Inference zamanı: 8.732 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Positional encoding is a technique used in neural network models, particularly in transformer architectures, to provide information about the position of tokens in a sequence. Since token embeddings are position-independent, positional encodings are added to these embeddings to incorporate positional context. There are various methods for positional encoding, including:\n",
      "\n",
      "1. **Learnable Positional Embeddings**: These are treated as learnable parameters and trained alongside other model parameters to create unique representations for each position.\n",
      "\n",
      "2. **Sinusoidal Encoding**: A common method that uses sine and cosine functions to generate positional encodings, enabling the model to generalize to sequences of different lengths.\n",
      "\n",
      "3. **Rotary Positional Embedding**: This method involves rotating the token embeddings in a complex space, allowing the model to capture relative positional information.\n",
      "\n",
      "4. **Extrapolation and Interpolation**: Techniques to generalize positional embeddings beyond the training sequence length, either by extrapolating learned embeddings to new positions or by interpolating positions to fit within the observed range.\n",
      "\n",
      "Overall, positional encoding is crucial for models to understand the order of tokens in a sequence, which is essential for tasks like language modeling and sequence processing.\n"
     ]
    }
   ],
   "source": [
    "query10 = \"What is positional encoding?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is semantic chunking and how does it improve search efficiency?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.637, Cosine: 0.767, Uzunluk: 3191) For example, LLMs can access real-time data from ﬁnancial markets to prov ide up-to-date investment ...\n",
      "2. (Hybrid Score: 0.546, Cosine: 0.780, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "3. (Hybrid Score: 0.544, Cosine: 0.777, Uzunluk: 3393) Long Papers) , pages 86–96, 2016. [Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, ...\n",
      "4. (Hybrid Score: 0.543, Cosine: 0.776, Uzunluk: 5572) This moti vates researchers to develop new evaluation benchmarks and metrics for long-context LLMs. ...\n",
      "5. (Hybrid Score: 0.541, Cosine: 0.773, Uzunluk: 3900) it remains challenging to effectively prompt LLMs. Note tha t if we face a very difﬁcult classiﬁca- ...\n",
      "\n",
      "Inference zamanı: 5.441 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Semantic chunking is the process of breaking down text into meaningful segments or \"chunks\" that represent coherent units of information. This technique enhances search efficiency by allowing search algorithms to focus on these meaningful segments rather than processing the entire text as a whole. By organizing information into chunks, it becomes easier to retrieve relevant data, as the search can target specific concepts or entities within the chunks. This can lead to faster and more accurate search results, as the system can better understand the context and relevance of the information being queried.\n"
     ]
    }
   ],
   "source": [
    "query12 = \"What is semantic chunking and how does it improve search efficiency?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendi point'leriniz başarıyla koleksiyona eklendi!\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Üç ayrı metin (chunk) tanımlıyoruz:\n",
    "custom_texts = [\n",
    "    \"The Transformer architecture is a fundamental building block in language models. Its self-attention mechanism enables the model to capture relationships between words.\",\n",
    "    \"Positional encoding assists models in understanding the order of words, ensuring that the sentence structure is maintained.\",\n",
    "    \"Semantic chunking splits long texts into meaningful segments, facilitating efficient search and information retrieval. This method helps in grouping similar content together.\"\n",
    "]\n",
    "\n",
    "points = []\n",
    "for text in custom_texts:\n",
    "    # Her chunk için embedding hesapla\n",
    "    vector = embeddings.embed_query(text)\n",
    "    \n",
    "    # TF-IDF ile anahtar kelimeleri çıkar (ilk 5 anahtar kelime)\n",
    "    custom_keywords = extract_keywords_tfidf(text, top_n=5)\n",
    "    \n",
    "    # Her chunk için point oluştur\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\n",
    "            \"text\": text,\n",
    "            \"keywords\": custom_keywords\n",
    "        }\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "# Tüm point'leri Qdrant koleksiyonuna ekle\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(\"Kendi point'leriniz başarıyla koleksiyona eklendi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What are the key features of the Transformer architecture??\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.706, Cosine: 0.866, Uzunluk: 167) The Transformer architecture is a fundamental building block in language models. Its self-attention ...\n",
      "2. (Hybrid Score: 0.556, Cosine: 0.794, Uzunluk: 6730) verge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many fa...\n",
      "3. (Hybrid Score: 0.547, Cosine: 0.782, Uzunluk: 3153) This can be expressed as Output = Merge(head 1,...,head τ)Whead(2.70) where head j∈Rdhis computed us...\n",
      "4. (Hybrid Score: 0.546, Cosine: 0.780, Uzunluk: 4147) An example 74 Generative Models of this approach is compressive Transformer [ Rae et al. ,2019 ]. It...\n",
      "5. (Hybrid Score: 0.546, Cosine: 0.780, Uzunluk: 1925) [Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011. [Kaplan et al., 2020] J...\n",
      "\n",
      "Inference zamanı: 9.256 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The key features of the Transformer architecture include:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This allows the model to capture relationships between words by weighing the importance of different words in a sequence, enabling it to focus on relevant parts of the input.\n",
      "\n",
      "2. **Layer Normalization**: Used to stabilize training, layer normalization reduces covariate shift and improves training stability, often combined with residual connections.\n",
      "\n",
      "3. **Residual Connections**: These connections help in training deep networks by allowing gradients to flow through the network without vanishing, facilitating the learning process.\n",
      "\n",
      "4. **Feed-Forward Networks (FFNs)**: Introduce non-linearities into representation learning, which helps prevent degeneration of representations learned by self-attention.\n",
      "\n",
      "5. **Activation Functions**: Common choices include ReLU, GeLU, and GLU-based functions, which are crucial for introducing non-linearity in FFNs.\n",
      "\n",
      "6. **Multi-Head Attention**: This allows the model to attend to different parts of the input simultaneously, enhancing its ability to capture complex relationships.\n",
      "\n",
      "7. **Positional Embeddings**: Since Transformers are order-insensitive, positional embeddings are added to token embeddings to encode the position of tokens in the sequence.\n",
      "\n",
      "8. **Scalability**: The architecture is designed to be scalable, making it suitable for training large language models (LLMs) efficiently.\n",
      "\n",
      "9. **Distributed Training**: Transformers can leverage large-scale distributed systems for training, enhancing computational efficiency.\n",
      "\n",
      "10. **Modifications\n"
     ]
    }
   ],
   "source": [
    "query8 = \"What are the key features of the Transformer architecture??\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is positional encoding?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.768, Cosine: 0.883, Uzunluk: 123) Positional encoding assists models in understanding the order of words, ensuring that the sentence s...\n",
      "2. (Hybrid Score: 0.744, Cosine: 0.849, Uzunluk: 5334) In this case, the embedding at position ican be expressed as ei=xi+ PE(i) (2.74) where xi∈Rddenotes ...\n",
      "3. (Hybrid Score: 0.718, Cosine: 0.811, Uzunluk: 1108) However, Press et al. [2022 ] found that setting βto values decreasing geometrically by a factor of1...\n",
      "4. (Hybrid Score: 0.711, Cosine: 0.802, Uzunluk: 7737) is the rotation matrix. If two or more rotations are performe d on the same vector, we can rotate th...\n",
      "5. (Hybrid Score: 0.571, Cosine: 0.816, Uzunluk: 2879) positions are represented as combinations of sine and cosin e functions with different frequencies. ...\n",
      "\n",
      "Inference zamanı: 3.307 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Positional encoding is a technique used in models, particularly in natural language processing, to help them understand the order of words in a sequence. It ensures that the sentence structure is maintained by adding a positional context to token embeddings. Each token embedding is combined with a positional embedding, allowing the model to distinguish tokens based on their positions in the sequence. This is crucial for maintaining the semantic meaning of sequences, especially when the order of tokens affects the overall interpretation of the text.\n"
     ]
    }
   ],
   "source": [
    "query9 = \"What is positional encoding?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is semantic chunking and how does it improve search efficiency?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.633, Cosine: 0.904, Uzunluk: 174) Semantic chunking splits long texts into meaningful segments, facilitating efficient search and info...\n",
      "2. (Hybrid Score: 0.564, Cosine: 0.806, Uzunluk: 123) Positional encoding assists models in understanding the order of words, ensuring that the sentence s...\n",
      "3. (Hybrid Score: 0.546, Cosine: 0.780, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "4. (Hybrid Score: 0.544, Cosine: 0.777, Uzunluk: 3393) Long Papers) , pages 86–96, 2016. [Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, ...\n",
      "5. (Hybrid Score: 0.543, Cosine: 0.776, Uzunluk: 5572) This moti vates researchers to develop new evaluation benchmarks and metrics for long-context LLMs. ...\n",
      "\n",
      "Inference zamanı: 2.347 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Semantic chunking is a method that splits long texts into meaningful segments, which helps in grouping similar content together. By organizing information into these coherent segments, semantic chunking facilitates more efficient search and information retrieval. This structured approach enables users to locate relevant information more quickly, as it reduces the cognitive load associated with processing large volumes of text and allows for targeted searches within specific segments.\n"
     ]
    }
   ],
   "source": [
    "query11 = \"What is semantic chunking and how does it improve search efficiency?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.875, Cosine: 0.821, Uzunluk: 2710) forgetting problem in continual training, where a neural network forge ts previously learned in- for...\n",
      "2. (Hybrid Score: 0.872, Cosine: 0.818, Uzunluk: 5110) The training objective can be deﬁned as (ˆθ,ˆω) = arg max θ,ωLoss(Model θ,ω(xnoise),x) (1.16) Here t...\n",
      "3. (Hybrid Score: 0.871, Cosine: 0.816, Uzunluk: 4839) The ﬁne-tuned model is then employed to classify new sequences for this task. An advantage of superv...\n",
      "4. (Hybrid Score: 0.866, Cosine: 0.809, Uzunluk: 9996) D dataset used for training or ﬁne-tuning a model ∂L ∂θgradient of the loss function Lwith respect t...\n",
      "5. (Hybrid Score: 0.857, Cosine: 0.796, Uzunluk: 3860) i+1for short). Suppose we have the gold- standard distribution at the same position, denoted by pgol...\n",
      "\n",
      "Inference zamanı: 3.397 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training is a machine learning approach where a model is initially trained on a large amount of unlabeled data to learn general representations or features before being fine-tuned on a specific task with labeled data. In the context of natural language processing (NLP), pre-training typically involves training models, such as neural networks, on tasks like masked language modeling, where the model learns to predict masked or missing words in a text. This foundational training allows the model to capture linguistic structures and general knowledge, which can then be adapted to various downstream tasks, enhancing performance and reducing the need for extensive labeled datasets.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.604, Cosine: 0.863, Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "2. (Hybrid Score: 0.592, Cosine: 0.846, Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "3. (Hybrid Score: 0.590, Cosine: 0.843, Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "4. (Hybrid Score: 0.589, Cosine: 0.842, Uzunluk: 3861) The use of these ex- amples does not distinguish between models, but we mark the m odel architecture...\n",
      "5. (Hybrid Score: 0.583, Cosine: 0.833, Uzunluk: 8279) in work using shared vocabularies, specifying the language to which a token belongs is not necessary...\n",
      "\n",
      "Inference zamanı: 5.488 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The types of models widely used in NLP pre-training include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)** - A Transformer encoder trained using masked language modeling and next sentence prediction tasks.\n",
      "2. **GPT (Generative Pre-trained Transformer)** - A generative model that focuses on predicting the next token in a sequence.\n",
      "3. **Transformer-based models** - General models that leverage the Transformer architecture for various language tasks.\n",
      "4. **LSTM-based models** - Long Short-Term Memory models that were previously popular for sequence representation before the rise of Transformers.\n",
      "5. **TinyBERT** - A distilled version of BERT aimed at improving efficiency while maintaining performance.\n",
      "6. **SpanBERT** - A variant of BERT that focuses on representing and predicting spans of text.\n",
      "\n",
      "These models are typically pre-trained on large-scale data and then fine-tuned for specific downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.682, Cosine: 0.831, Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "2. (Hybrid Score: 0.681, Cosine: 0.830, Uzunluk: 7725) sense to predict any of the tokens in this sequence. 1.2.2.1 Masked Language Modeling One of the mos...\n",
      "3. (Hybrid Score: 0.673, Cosine: 0.818, Uzunluk: 3860) i+1for short). Suppose we have the gold- standard distribution at the same position, denoted by pgol...\n",
      "4. (Hybrid Score: 0.582, Cosine: 0.831, Uzunluk: 8453) The approach described above provides a new framework of uni versal language understanding and gener...\n",
      "5. (Hybrid Score: 0.578, Cosine: 0.826, Uzunluk: 5839) architecture to adapt LLMs to large-scale training. In Sect ion2.2we will present more discussions o...\n",
      "\n",
      "Inference zamanı: 9.554 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling is implemented by allowing the model to predict tokens in a sequence in a non-sequential order while still maintaining the original order of the tokens in the text. Here’s how it works:\n",
      "\n",
      "1. **Token Sequence Preparation**: Start with a sequence of tokens, for example, \\(x_0, x_1, x_2, x_3, x_4\\).\n",
      "\n",
      "2. **Define Prediction Order**: Instead of predicting tokens in the natural left-to-right order, define a new order for predictions. For instance, you might choose the order \\(x_0, x_4, x_2, x_1, x_3\\).\n",
      "\n",
      "3. **Modeling the Probability**: The probability of the sequence can be modeled through a generation process that reflects the new prediction order. For example:\n",
      "   \\[\n",
      "   Pr(x) = Pr(x_0) \\cdot Pr(x_4|e_0) \\cdot Pr(x_2|e_0, e_4) \\cdot Pr(x_1|e_0, e_4, e_2) \\cdot Pr(x_3|e_0, e_4, e_2, e_1)\n",
      "   \\]\n",
      "   Here, \\(e_i\\) represents the embeddings of the tokens \\(x_i\\).\n",
      "\n",
      "4. **Self-Attention Mechanism**: In a Transformer model, implement this by using masking in the self\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar (hybrid sıralama ile):\n",
      "1. (Hybrid Score: 0.784, Cosine: 0.835, Uzunluk: 2710) forgetting problem in continual training, where a neural network forge ts previously learned in- for...\n",
      "2. (Hybrid Score: 0.778, Cosine: 0.826, Uzunluk: 9996) D dataset used for training or ﬁne-tuning a model ∂L ∂θgradient of the loss function Lwith respect t...\n",
      "3. (Hybrid Score: 0.777, Cosine: 0.825, Uzunluk: 9182) example, in He et al. [2021 ]’s work, a 1.5 billion-parameter BERT-like model is built b y increasin...\n",
      "4. (Hybrid Score: 0.683, Cosine: 0.833, Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "5. (Hybrid Score: 0.682, Cosine: 0.832, Uzunluk: 4869) scaling laws for LLMs, which help us understand their traini ng efﬁciency and effectiveness. 2.2.1 D...\n",
      "\n",
      "Inference zamanı: 4.935 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The large-scale pre-training discussed in the document refers to the process of training neural sequence models, particularly in natural language processing (NLP), on vast amounts of unlabeled data using self-supervised learning techniques. This approach allows models like BERT and GPT to learn general language understanding and generation capabilities by predicting masked words in large text corpora. These pre-trained models serve as foundation models that can be easily adapted to various downstream tasks through fine-tuning or prompting, significantly changing the paradigm of NLP by reducing the need for large-scale supervised learning for specific tasks. The document highlights the effectiveness of pre-training in improving model performance across a wide range of NLP problems and emphasizes the importance of data quality and diversity in this process.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_hybrid_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
